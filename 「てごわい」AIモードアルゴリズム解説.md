この解説は OpenAIの対話型AI「ChatGPT」を使用して製作しています。

# 🎯 リバーシ風ゲームにおけるAIアルゴリズム解説

「てごわい」AIモードで使用されている **ミニマックス法 + α–β枝刈り法** について解説です。

---

## 🔍 使用しているアルゴリズム

### 1. ミニマックス法（Minimax）

ミニマックス法は、**完全情報ゲーム**（リバーシ、将棋、チェスなど）で使われる探索手法です。

このゲームでは、

* **AIが最も有利になる手（最大化）** を選び、
* **相手が最もAIにとって不利な手（最小化）** を選ぶと仮定して、

すべての可能な手を探索します。

#### ✅ 特徴

* AIは「自分にとって最大スコアの手」を選ぶ（MAX）
* プレイヤーは「AIにとって最小スコアの手」を選ぶ（MIN）
* これを交互に繰り返すため **minimax（最小-最大）** と呼ばれます

#### 💡 基本イメージ

```
AI（白）の手番（MAX）
├─ プレイヤー（黒）の手番（MIN）
│   ├─ AI（白）の手番（MAX）
│   │   └─ ... （depthが0になるまで）
```

これを **再帰的** に繰り返し、`depth`（探索深さ）で制限します。

---

## 🧩 `minimax_ab()` 関数のステップ分解（図付き）

```
例）AI（白）のターン → get_best_move_ab(board, depth=2)

                          AI(MAX)
                          ↓
           ┌───────────────┬───────────────┐
         黒(MIN)                      黒(MIN)
      ┌─────┐              ┌─────┐
   白(MAX) 白(MAX)        白(MAX) 白(MAX)

1. AI が有効手を仮想で打つ → 相手（MIN）へ渡す
2. 相手が有効手を仮想で打つ → 再びAI（MAX）へ渡す
3. `depth=0` になると `evaluate_board()` で盤面をスコア化
4. スコアを親ノードへ返す → MAXは最大値、MINは最小値を選ぶ
```

※これは簡略図です。実際の分岐数は盤面状況によって変化します。

---

### 2. α–β枝刈り法（Alpha-Beta Pruning）

ミニマックス法は強力ですが、すべての手を読むと処理が重くなります。

そこで導入されるのが **α–β法（枝刈り）**です。
これはミニマックス法の **高速化手法** であり、無駄な探索を途中で打ち切ります。

#### ✅ キーワード

* **α（アルファ）**：最大化側（AI）で既に見つかっている最良のスコア（下限）
* **β（ベータ）**：最小化側（プレイヤー）で既に見つかっている最悪のスコア（上限）

#### ✅ 枝刈りの仕組み

* AIが探索中「この手以上の結果が見込めない」と分かったら `βカット`
* プレイヤー側が「これ以下には絶対ならない」と分かったら `αカット`

```python
if beta <= alpha:
    break  # 枝刈り！
```

---

## 🧠 評価関数：`evaluate_board(board, ai_color)`

AIの視点から、盤面の良し悪しを数値（スコア）で評価する関数です。

```python
# --- 盤面評価関数（ミニマックス用） ---
def evaluate_board(board, ai_color):
    opponent = 1 if ai_color == 2 else 2  # AIが2ならプレイヤーは1、AIが1ならプレイヤーは2（相手の色）
    score = 0 # 評価値（AIにとって有利かどうか）を初期化
    corners = [(0,0),(0,7),(7,0),(7,7)] # 角のマス（高評価ポイント）
    xs      = [(1,1),(1,6),(6,1),(6,6)] # 角の隣のXマス（危険ポイント）

    for y in range(ROWS):  # 盤面の行
        for x in range(COLS):  # 盤面の列
            cell = board[y][x]  # 現在のマスの内容（0:空, 1:黒, 2:白）

            if cell == ai_color: 
                # AIの石 → +1（石の数）＋ 位置による補正（角なら+100、Xなら-30、それ以外は0）
                score += 1 + (100 if (y,x) in corners else -30 if (y,x) in xs else 0)

            elif cell == opponent: 
                # 相手の石 → -1（石の数）＋ 位置による補正（角なら-100、Xなら+30、それ以外は0）
                score -= 1 + (100 if (y,x) in corners else -30 if (y,x) in xs else 0)

    return score # 最終的な評価値を返す（正数ならAIが有利、負数なら不利）
```

#### 💡 評価基準（簡易）

* 石の数：1個 = +1 点
* 角：+100 点（超有利）
* Xの位置（角の隣）：-30 点（危険）
* 相手の石はスコアを減点

> 📘 評価関数は AI の「価値観」を決める採点ルールです。

---

## 🔍 ログの読み方チュートリアル

```text
MIN: (2,1) を仮想で置く → 深さ 1
  MAX: (1,1) を仮想で置く → 深さ 0
    depth=0, MIN: 終端→ score=-30
  MAX: (1,1) の評価値 = -30
  MAX: (4,2) を仮想で置く → 深さ 0
    depth=0, MIN: 終端→ score=2
  MAX: (4,2) の評価値 = 2
  MAX: βカット発生！（β=0 <= α=2）
  MAX: 最終スコア = 2
MIN: (2,1) の評価値 = 2
```

### 🗂 用語解説

* `MIN`: プレイヤーが動くターン（AIにとって最悪を選ぶ）
* `MAX`: AIが動くターン（最良を選ぶ）
* `depth=0`: 探索終了。このタイミングで評価関数（`evaluate_board()`）を使ってスコアを計算
* `score=...`: `evaluate_board()` によって計算されたその盤面のスコア
* `MAX: (x,y) の評価値 = n`: その手を選んだ結果得られた評価スコア（再帰の戻り値）
* `最終スコア = n`: 複数の手を見た中で、MAXなら最大値、MINなら最小値として選んだ値

### 🔪 α/βカットの詳しい意味

* `α（アルファ）`：MAX（AI側）にとって「現時点で保証された最低スコア」
* `β（ベータ）`：MIN（プレイヤー側）にとって「現時点で保証された最高スコア」

#### ✅ αカット・βカットとは？

* **MAXのターン**では「もうこれ以上良い手は出ない」と判断したら `βカット` を発動
* **MINのターン**では「もうこれ以上悪い手は出ない」と判断したら `αカット` を発動

```text
例：
MAX: (4,2) の評価値 = 2
MAX: βカット発生！（β=0 <= α=2）
```

この例では、すでに `α=2` という「良いスコア」が見つかっており、
次の手を見た結果 `β=0` となり `β <= α` の条件を満たしたため、**それ以上の探索を打ち切った**という意味です。

結果として、`MAX` の「最終スコア = 2」がこのノードで選ばれ、親ノード（MIN側）へ返されます。

※カットされた以降の手は「見ても意味がない」と判断されるため、再帰処理されずスキップされます。

---

## 🔧 実装構造のポイント

### `minimax_ab(board, depth, alpha, beta, maximizing_player, ai_color)`

AIの再帰的な読みを実現する中核関数です。

| 引数                  | 内容                       |
| ------------------- | ------------------------ |
| `board`             | 現在の盤面（2次元リスト）            |
| `depth`             | 残りの探索深さ                  |
| `alpha`             | 最大化側の保証スコア下限             |
| `beta`              | 最小化側の保証スコア上限             |
| `maximizing_player` | AIターン: True／プレイヤー: False |
| `ai_color`          | AIの色（白=2 or 黒=1）         |

#### 🔁 処理の流れ

1. `depth == 0` または手がない → `evaluate_board()` を呼ぶ（終端）
2. 各手を仮想で置いて `minimax_ab()` を再帰呼び出し
3. 最大化なら `max()`、最小化なら `min()` でスコア選定
4. α・βを更新、枝刈りが発生する場合 `break`

---

### `get_best_move_ab(board, depth, ai_color)`

実際に「AIが次に打つ手」を決定する関数です。

1. 有効手をすべて取得
2. 各手に `minimax_ab()` を適用しスコア算出
3. 最も高評価の手を収集
4. 複数ある場合はランダムに選んで返す

---

## 🎮 難易度「てごわい」で使用

```python
def ai_move_hard():
    move = get_best_move_ab(board, depth=3, ai_color=2)
    ...
```

探索深度は `depth=3` に設定（3手先まで読む）
これ以上の深さに設定すればAIはさらに強くなりますが、**計算時間が大きく増加**します。

⚠️ **depth=7以上では処理が非常に重くなり、応答が停止したように見える可能性があります。**
特に `depth=10` に設定すると、**私の環境ではフリーズしてタスクマネージャーから強制終了する事態が発生しました。**
慎重に調整してください。

---

## 🧠 α–β枝刈り：イメージで理解

```
AI（自分） → プレイヤー → AI → ...
（MAXとMINが交互）

探索ツリー（depth=2）:
     AI
    /  \
  P1    P2     ← プレイヤーの手
 / \   / \
A  B  C  D     ← AIが再び選ぶ手
```

* AIは「最良の手」を探す
* プレイヤーは「AIにとって最悪の手」を選ぶと仮定
* α–β法により「この先はどうせ悪いからスキップ」となる場面が多々ある

```text
例：
ある手で評価値が +100 出たあと、別の手が -50 だったら、
「この手はそれ以下にしかならない」と判断 → 枝を刈る（探索を打ち切る）
```

---

## 📝 補足メモ

* **このAIは完全読みではありません。**
  評価関数と探索深度の工夫で「賢く見える」ように設計しています。

* **この実装はあくまで簡易なものです。**
  安定石、可動性、辺の制御といった高度な要素は未導入です。

---

## ✅ ライセンス

この解説は MIT ライセンスのもとで公開されているコードの補足文書です。ご自由にご利用ください。
